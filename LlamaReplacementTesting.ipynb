{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_router_with_memory.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, TypedDict\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema import BaseMemory\n",
    "from langchain.memory import ConversationBufferMemory, CombinedMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chat_models import ChatOpenAI  # only if you still want OpenAI elsewhere\n",
    "from langchain.agents import Tool, AgentType, create_pandas_dataframe_agent\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# â€”â€”â€” 1) Replace LlamaCpp with a HuggingFace pipeline â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "\n",
    "from transformers import pipeline, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# 1a) point this at your local HF directory (or model name on HF Hub)\n",
    "MODEL_PATH = \"/path/to/your/llama-3.2-3b-checkpoint/\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model     = LlamaForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "# build a standard text-generation pipeline\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# wrap it for LangChain\n",
    "hf_llm = HuggingFacePipeline(pipeline=text_gen)\n",
    "\n",
    "\n",
    "# â€”â€”â€” 2) Tools you still want to keep â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "\n",
    "def calculator_tool(expr: str) -> str:\n",
    "    try:\n",
    "        return f\"The result of '{expr}' is {eval(expr)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "calculator = Tool(\"Calculator\", calculator_tool, \"Basic arithmetic\")\n",
    "\n",
    "def sentiment_analysis_tool(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    if any(w in t for w in (\"happy\",\"great\",\"good\",\"excellent\")):\n",
    "        return \"Positive ğŸ‘\"\n",
    "    if any(w in t for w in (\"sad\",\"bad\",\"terrible\",\"awful\")):\n",
    "        return \"Negative ğŸ‘\"\n",
    "    return \"Neutral ğŸ¤”\"\n",
    "sentiment = Tool(\"SentimentAnalysis\", sentiment_analysis_tool, \"Sentiment detection\")\n",
    "\n",
    "\n",
    "# â€”â€”â€” 3) Build your Pandas-DataFrame agent with the same HF LLM â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "\n",
    "# assuming `df` is already defined earlier in your script\n",
    "pandas_agent = create_pandas_dataframe_agent(\n",
    "    hf_llm,         # use HuggingFacePipeline instead of ChatOpenAI\n",
    "    df,\n",
    "    verbose=True,\n",
    "    allow_dangerous_code=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,  # or your preferred type\n",
    ")\n",
    "\n",
    "\n",
    "# â€”â€”â€” 4) Memory & caching scaffolding â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "\n",
    "_long_term: Dict[str,List[str]] = {}\n",
    "_qa_cache:    Dict[str,str]   = {}\n",
    "_qa_embeds:   Dict[str,List[float]] = {}\n",
    "SESSION_ID = \"user_123\"\n",
    "\n",
    "def update_long_term_memory(user: str, inp: str, outp: str):\n",
    "    mem = _long_term.setdefault(user, [])\n",
    "    if inp:  mem.append(f\"User: {inp}\")\n",
    "    if outp: mem.append(f\"Bot:  {outp}\")\n",
    "    _long_term[user] = mem[-10:]\n",
    "\n",
    "def get_long_term_memory(user: str) -> str:\n",
    "    return \"\\n\".join(_long_term.get(user, []))\n",
    "\n",
    "EMB = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "def find_similar_cached(q: str, threshold: float=0.85) -> str:\n",
    "    vec = EMB.embed_query(q)\n",
    "    for orig,ov in _qa_embeds.items():\n",
    "        if cosine_similarity([vec],[ov])[0][0] >= threshold:\n",
    "            return orig\n",
    "    return None\n",
    "\n",
    "TOOL_KEYWORDS = [\"calculate\",\"translate\",\"search\",\"find\",\"what\",\"give me\",\"list\",\"summarize\",\"analyze\"]\n",
    "def is_tool_query(txt: str) -> bool:\n",
    "    low = txt.lower().strip()\n",
    "    if \";\" in txt or \"\\n\" in txt: return True\n",
    "    return any(low.startswith(kw) for kw in TOOL_KEYWORDS)\n",
    "\n",
    "\n",
    "# â€”â€”â€” 5) Router via LangGraph (unchanged except we only keep 3 tools) â€”â€”â€”â€”\n",
    "\n",
    "class RouterState(TypedDict):\n",
    "    input: str\n",
    "    decision: str\n",
    "    output: Any\n",
    "\n",
    "def llm_call_router(state: RouterState) -> RouterState:\n",
    "    t = state[\"input\"].strip().lower()\n",
    "    if t.startswith(\"calculate\"):    state[\"decision\"] = \"Calculator\"\n",
    "    elif \"sentiment\" in t or any(w in t for w in (\"happy\",\"sad\")):\n",
    "        state[\"decision\"] = \"SentimentAnalysis\"\n",
    "    else:\n",
    "        state[\"decision\"] = \"PandasDataAnalysis\"\n",
    "    return state\n",
    "\n",
    "def run_Calculator(st):    st[\"output\"]=calculator_tool(st[\"input\"]); return st\n",
    "def run_Sentiment(st):     st[\"output\"]=sentiment_analysis_tool(st[\"input\"]); return st\n",
    "def run_Pandas(st):        st[\"output\"]=pandas_agent.run(st[\"input\"]); return st\n",
    "\n",
    "router = StateGraph(state_schema=RouterState)\n",
    "router.add_node(\"router\", llm_call_router)\n",
    "router.add_node(\"Calculator\", run_Calculator)\n",
    "router.add_node(\"SentimentAnalysis\", run_Sentiment)\n",
    "router.add_node(\"PandasDataAnalysis\", run_Pandas)\n",
    "router.add_edge(START, \"router\")\n",
    "router.add_conditional_edges(\"router\", lambda s: s[\"decision\"],\n",
    "                            {\"Calculator\":\"Calculator\",\n",
    "                             \"SentimentAnalysis\":\"SentimentAnalysis\",\n",
    "                             \"PandasDataAnalysis\":\"PandasDataAnalysis\"})\n",
    "for node in (\"Calculator\",\"SentimentAnalysis\",\"PandasDataAnalysis\"):\n",
    "    router.add_edge(node, END)\n",
    "router.set_entry_point(\"router\")\n",
    "router_workflow = router.compile()\n",
    "\n",
    "\n",
    "# â€”â€”â€” 6) Pure-chat chain, now on HF Llama 3.2 â€3B â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful AI assistant.\"),\n",
    "    (\"system\",\"Chat history:\\n{chat_history}\"),\n",
    "    (\"system\",\"Long-term memory:\\n{long_term_memory}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\",\"{input}\")\n",
    "])\n",
    "\n",
    "short_term = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"input\", return_messages=True)\n",
    "long_term  = CombinedMemory  # placeholder not used here\n",
    "combined  = CombinedMemory(memories=[short_term,\n",
    "                LongTermChatMemory(session_id=SESSION_ID)])\n",
    "\n",
    "chat_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hf_llm,   # our HuggingFacePipeline\n",
    "    memory=combined\n",
    ")\n",
    "\n",
    "\n",
    "# â€”â€”â€” 7) unify in `converse()` â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "\n",
    "def converse(user_input: str) -> str:\n",
    "    low = user_input.lower().strip()\n",
    "    if \"remember\" in low:\n",
    "        return \"Memory:\\n\"+get_long_term_memory(SESSION_ID)\n",
    "\n",
    "    if is_tool_query(user_input):\n",
    "        sim = find_similar_cached(user_input)\n",
    "        if sim:\n",
    "            return _qa_cache[sim]\n",
    "        st  = router_workflow.invoke({\"input\": user_input})\n",
    "        out = st[\"output\"]\n",
    "        _qa_cache[user_input]  = out\n",
    "        _qa_embeds[user_input] = EMB.embed_query(user_input)\n",
    "        update_long_term_memory(SESSION_ID, user_input, out)\n",
    "        return out\n",
    "\n",
    "    out = chat_chain.run(user_input)\n",
    "    update_long_term_memory(SESSION_ID, user_input, out)\n",
    "    return out\n",
    "\n",
    "\n",
    "# â€”â€”â€” 8) quick smokeâ€test â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for q in [\n",
    "        \"Calculate 3 * 8\",\n",
    "        \"How am I feeling? Sentiment 'I am sad'\",\n",
    "        \"Show me summary stats on the DataFrame\",\n",
    "        \"Do you remember our previous requests?\"\n",
    "    ]:\n",
    "        print(\"â†’\", converse(q))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

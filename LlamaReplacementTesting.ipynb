{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_router_with_memory.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, TypedDict\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema import BaseMemory\n",
    "from langchain.memory import ConversationBufferMemory, CombinedMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import Tool, AgentType, create_pandas_dataframe_agent\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 1) Llama 3.2-3B local HF loading\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "MODEL_PATH = \"/path/to/your/llama-3.2-3b-checkpoint\"\n",
    "# force local-only so it won’t recurse into unwanted subfolders:\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    MODEL_PATH, trust_remote_code=True, local_files_only=True\n",
    ")\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "hf_llm = HuggingFacePipeline(pipeline=text_gen)\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 2) Define your 3 tools\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "def calculator_tool(expr: str) -> str:\n",
    "    try:\n",
    "        return f\"The result of '{expr}' is {eval(expr)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "calculator = Tool(\"Calculator\", calculator_tool, \"Basic arithmetic\")\n",
    "\n",
    "def sentiment_analysis_tool(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    if any(w in t for w in (\"happy\",\"great\",\"good\",\"excellent\")):\n",
    "        return \"Positive 👍\"\n",
    "    if any(w in t for w in (\"sad\",\"bad\",\"terrible\",\"awful\")):\n",
    "        return \"Negative 👎\"\n",
    "    return \"Neutral 🤔\"\n",
    "sentiment = Tool(\"SentimentAnalysis\", sentiment_analysis_tool, \"Detect sentiment\")\n",
    "\n",
    "# Pandas agent: uses the same hf_llm under the hood\n",
    "# assume `df` is defined elsewhere in your script\n",
    "pandas_agent = create_pandas_dataframe_agent(\n",
    "    hf_llm,\n",
    "    df,\n",
    "    verbose=True,\n",
    "    allow_dangerous_code=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    ")\n",
    "def run_pandas(q: str) -> str:\n",
    "    return pandas_agent.run(q)\n",
    "pandas = Tool(\"PandasDataAnalysis\", run_pandas, \"Query Pandas DataFrame\")\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 3) Episodic Memory & Semantic Cache\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "_long_term: Dict[str,List[str]]    = {}\n",
    "_qa_cache:    Dict[str,str]        = {}\n",
    "_qa_embeds:   Dict[str,np.ndarray] = {}\n",
    "SESSION_ID = \"user_123\"\n",
    "\n",
    "def update_long_term_memory(user: str, inp: str, outp: str):\n",
    "    mem = _long_term.setdefault(user, [])\n",
    "    if inp:  mem.append(f\"User: {inp}\")\n",
    "    if outp: mem.append(f\"Bot:  {outp}\")\n",
    "    _long_term[user] = mem[-10:]\n",
    "\n",
    "def get_long_term_memory(user: str) -> str:\n",
    "    return \"\\n\".join(_long_term.get(user, []))\n",
    "\n",
    "\n",
    "# load sentence-transformers locally, no recursion\n",
    "EMB_MODEL = SentenceTransformer(\n",
    "    \"/path/to/your/all-MiniLM-L6-v2\", \n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "def find_similar_cached(q: str, threshold: float = 0.85) -> str:\n",
    "    vec = EMB_MODEL.encode(q, convert_to_numpy=True)\n",
    "    for orig, ov in _qa_embeds.items():\n",
    "        sim = cosine_similarity(vec.reshape(1,-1), ov.reshape(1,-1))[0][0]\n",
    "        if sim >= threshold:\n",
    "            return orig\n",
    "    return None\n",
    "\n",
    "\n",
    "TOOL_KEYWORDS = [\"calculate\",\"sentiment\",\"pandas\",\"data\"]\n",
    "def is_tool_query(txt: str) -> bool:\n",
    "    low = txt.lower().strip()\n",
    "    return any(low.startswith(kw) for kw in TOOL_KEYWORDS)\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 4) LangGraph router for those 3 tools\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "class RouterState(TypedDict):\n",
    "    input: str\n",
    "    decision: str\n",
    "    output: Any\n",
    "\n",
    "def llm_call_router(state: RouterState) -> RouterState:\n",
    "    t = state[\"input\"].strip().lower()\n",
    "    if t.startswith(\"calculate\"):\n",
    "        state[\"decision\"] = \"Calculator\"\n",
    "    elif any(w in t for w in (\"happy\",\"sad\",\"sentiment\")):\n",
    "        state[\"decision\"] = \"SentimentAnalysis\"\n",
    "    else:\n",
    "        state[\"decision\"] = \"PandasDataAnalysis\"\n",
    "    return state\n",
    "\n",
    "def run_Calculator(s):    s[\"output\"]=calculator_tool(s[\"input\"]); return s\n",
    "def run_Sentiment(s):     s[\"output\"]=sentiment_analysis_tool(s[\"input\"]); return s\n",
    "def run_Pandas(s):        s[\"output\"]=run_pandas(s[\"input\"]);            return s\n",
    "\n",
    "router = StateGraph(state_schema=RouterState)\n",
    "router.add_node(\"router\", llm_call_router)\n",
    "router.add_node(\"Calculator\", run_Calculator)\n",
    "router.add_node(\"SentimentAnalysis\", run_Sentiment)\n",
    "router.add_node(\"PandasDataAnalysis\", run_Pandas)\n",
    "\n",
    "router.add_edge(START, \"router\")\n",
    "router.add_conditional_edges(\"router\", lambda s: s[\"decision\"], {\n",
    "    \"Calculator\":\"Calculator\",\n",
    "    \"SentimentAnalysis\":\"SentimentAnalysis\",\n",
    "    \"PandasDataAnalysis\":\"PandasDataAnalysis\"\n",
    "})\n",
    "for node in (\"Calculator\",\"SentimentAnalysis\",\"PandasDataAnalysis\"):\n",
    "    router.add_edge(node, END)\n",
    "\n",
    "router.set_entry_point(\"router\")\n",
    "router_workflow = router.compile()\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 5) Pure‐chat Llama chain + memory\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful AI assistant.\"),\n",
    "    (\"system\",\"Chat history:\\n{chat_history}\"),\n",
    "    (\"system\",\"Long-term memory:\\n{long_term_memory}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\",\"{input}\"),\n",
    "])\n",
    "\n",
    "short_term = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"input\",\n",
    "    return_messages=True\n",
    ")\n",
    "long_term = CombinedMemory([short_term, LongTermChatMemory(session_id=SESSION_ID)])\n",
    "\n",
    "chat_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hf_llm,\n",
    "    memory=long_term\n",
    ")\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 6) Unified converse()\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "def converse(user_input: str) -> str:\n",
    "    low = user_input.lower().strip()\n",
    "    if \"remember\" in low:\n",
    "        return \"Memory:\\n\" + get_long_term_memory(SESSION_ID)\n",
    "\n",
    "    if is_tool_query(user_input):\n",
    "        # semantic cache\n",
    "        sim = find_similar_cached(user_input)\n",
    "        if sim:\n",
    "            return _qa_cache[sim]\n",
    "        st  = router_workflow.invoke({\"input\": user_input})\n",
    "        out = st[\"output\"]\n",
    "        _qa_cache[user_input]    = out\n",
    "        _qa_embeds[user_input]   = EMB_MODEL.encode(user_input, convert_to_numpy=True)\n",
    "        update_long_term_memory(SESSION_ID, user_input, out)\n",
    "        return out\n",
    "\n",
    "    # fallback to pure chat\n",
    "    out = chat_chain.run(user_input)\n",
    "    update_long_term_memory(SESSION_ID, user_input, out)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 7) Smoke‐test\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for q in [\n",
    "        \"Calculate 3 * 8\",\n",
    "        \"How am I feeling? Sentiment 'I am sad'\",\n",
    "        \"Show me summary stats on the DataFrame\",\n",
    "        \"Do you remember our previous requests?\"\n",
    "    ]:\n",
    "        print(\"→\", converse(q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc1cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_router_llm_driven.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import Any, Dict, TypedDict\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import Tool\n",
    "from langchain.schema import BaseMemory  # still required by LLMChain signature\n",
    "\n",
    "from transformers import pipeline, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 1) Spin up your local Llama 3.2-3B via HF pipeline\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "MODEL_PATH = \"/path/to/your/llama-3.2-3b-checkpoint/\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model     = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "hf_llm = HuggingFacePipeline(pipeline=text_gen)\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 2) Define your two real tools\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "def calculator_tool(expr: str) -> str:\n",
    "    try:\n",
    "        return f\"The result of '{expr}' is {eval(expr)}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "calculator = Tool(\"Calculator\", calculator_tool, \"Basic arithmetic\")\n",
    "\n",
    "def sentiment_analysis_tool(text: str) -> str:\n",
    "    low = text.lower()\n",
    "    if any(w in low for w in (\"happy\",\"great\",\"good\",\"excellent\")):\n",
    "        return \"Positive 👍\"\n",
    "    if any(w in low for w in (\"sad\",\"bad\",\"terrible\",\"awful\")):\n",
    "        return \"Negative 👎\"\n",
    "    return \"Neutral 🤔\"\n",
    "sentiment = Tool(\"SentimentAnalysis\", sentiment_analysis_tool, \"Detect sentiment\")\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 3) Stateless Chat chain (our fallback “tool”)\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"system\", \"If nothing else, just answer conversationally.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),  # will always be empty here\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "chat_chain = LLMChain(prompt=chat_prompt, llm=hf_llm)\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 4) LLM-driven router chain\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are a routing assistant.  Given a user query, \"\n",
    "     \"choose exactly one of: Calculator, SentimentAnalysis, Chat.  \"\n",
    "     \"Respond with only that tool name.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "router_chain = LLMChain(prompt=router_prompt, llm=hf_llm)\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 5) LangGraph router wiring\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "class RouterState(TypedDict):\n",
    "    input: str\n",
    "    decision: str\n",
    "    output: Any\n",
    "\n",
    "def llm_call_router(state: RouterState) -> RouterState:\n",
    "    # ask our LLM which tool to use\n",
    "    tool_name = router_chain.run(input=state[\"input\"]).strip()\n",
    "    # safe-guard: default to Chat if unexpected\n",
    "    if tool_name not in (\"Calculator\", \"SentimentAnalysis\", \"Chat\"):\n",
    "        tool_name = \"Chat\"\n",
    "    state[\"decision\"] = tool_name\n",
    "    return state\n",
    "\n",
    "def run_Calculator(state: RouterState) -> RouterState:\n",
    "    # strip leading command if present\n",
    "    expr = re.sub(r'^(calculate|calc)\\s+', '', state[\"input\"], flags=re.IGNORECASE)\n",
    "    state[\"output\"] = calculator_tool(expr)\n",
    "    return state\n",
    "\n",
    "def run_SentimentAnalysis(state: RouterState) -> RouterState:\n",
    "    state[\"output\"] = sentiment_analysis_tool(state[\"input\"])\n",
    "    return state\n",
    "\n",
    "def run_Chat(state: RouterState) -> RouterState:\n",
    "    state[\"output\"] = chat_chain.run(input=state[\"input\"])\n",
    "    return state\n",
    "\n",
    "router = StateGraph(state_schema=RouterState)\n",
    "router.add_node(\"router\", llm_call_router)\n",
    "router.add_node(\"Calculator\", run_Calculator)\n",
    "router.add_node(\"SentimentAnalysis\", run_SentimentAnalysis)\n",
    "router.add_node(\"Chat\", run_Chat)\n",
    "\n",
    "router.add_edge(START, \"router\")\n",
    "router.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda s: s[\"decision\"],\n",
    "    {\n",
    "        \"Calculator\": \"Calculator\",\n",
    "        \"SentimentAnalysis\": \"SentimentAnalysis\",\n",
    "        \"Chat\": \"Chat\"\n",
    "    }\n",
    ")\n",
    "for node in (\"Calculator\", \"SentimentAnalysis\", \"Chat\"):\n",
    "    router.add_edge(node, END)\n",
    "\n",
    "router.set_entry_point(\"router\")\n",
    "router_workflow = router.compile()\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 6) Unified entrypoint\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "def converse(user_input: str) -> str:\n",
    "    # route every query\n",
    "    st = router_workflow.invoke({\"input\": user_input})\n",
    "    return st[\"output\"]\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 7) Quick smoke-test\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for q in [\n",
    "        \"Calculate 3 * 8\",\n",
    "        \"How am I feeling? I am sad today.\",\n",
    "        \"What is the capital of France?\",\n",
    "        \"Tell me a joke\"\n",
    "    ]:\n",
    "        print(f\">>> {q!r}\")\n",
    "        print(\"→\", converse(q), \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_router_with_memory.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List, TypedDict\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.schema import BaseMemory\n",
    "from langchain.memory import ConversationBufferMemory, CombinedMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import Tool, AgentType, create_pandas_dataframe_agent\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 1) Llama 3.2-3B local HF loading\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "MODEL_PATH = \"/path/to/your/llama-3.2-3b-checkpoint\"\n",
    "# force local-only so it won’t recurse into unwanted subfolders:\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    MODEL_PATH, trust_remote_code=True, local_files_only=True\n",
    ")\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "hf_llm = HuggingFacePipeline(pipeline=text_gen)\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 2) Define your 3 tools\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "def calculator_tool(expr: str) -> str:\n",
    "    try:\n",
    "        return f\"The result of '{expr}' is {eval(expr)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "calculator = Tool(\"Calculator\", calculator_tool, \"Basic arithmetic\")\n",
    "\n",
    "def sentiment_analysis_tool(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    if any(w in t for w in (\"happy\",\"great\",\"good\",\"excellent\")):\n",
    "        return \"Positive 👍\"\n",
    "    if any(w in t for w in (\"sad\",\"bad\",\"terrible\",\"awful\")):\n",
    "        return \"Negative 👎\"\n",
    "    return \"Neutral 🤔\"\n",
    "sentiment = Tool(\"SentimentAnalysis\", sentiment_analysis_tool, \"Detect sentiment\")\n",
    "\n",
    "# Pandas agent: uses the same hf_llm under the hood\n",
    "# assume `df` is defined elsewhere in your script\n",
    "pandas_agent = create_pandas_dataframe_agent(\n",
    "    hf_llm,\n",
    "    df,\n",
    "    verbose=True,\n",
    "    allow_dangerous_code=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    ")\n",
    "def run_pandas(q: str) -> str:\n",
    "    return pandas_agent.run(q)\n",
    "pandas = Tool(\"PandasDataAnalysis\", run_pandas, \"Query Pandas DataFrame\")\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 3) Episodic Memory & Semantic Cache\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "_long_term: Dict[str,List[str]]    = {}\n",
    "_qa_cache:    Dict[str,str]        = {}\n",
    "_qa_embeds:   Dict[str,np.ndarray] = {}\n",
    "SESSION_ID = \"user_123\"\n",
    "\n",
    "def update_long_term_memory(user: str, inp: str, outp: str):\n",
    "    mem = _long_term.setdefault(user, [])\n",
    "    if inp:  mem.append(f\"User: {inp}\")\n",
    "    if outp: mem.append(f\"Bot:  {outp}\")\n",
    "    _long_term[user] = mem[-10:]\n",
    "\n",
    "def get_long_term_memory(user: str) -> str:\n",
    "    return \"\\n\".join(_long_term.get(user, []))\n",
    "\n",
    "\n",
    "# load sentence-transformers locally, no recursion\n",
    "EMB_MODEL = SentenceTransformer(\n",
    "    \"/path/to/your/all-MiniLM-L6-v2\", \n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "def find_similar_cached(q: str, threshold: float = 0.85) -> str:\n",
    "    vec = EMB_MODEL.encode(q, convert_to_numpy=True)\n",
    "    for orig, ov in _qa_embeds.items():\n",
    "        sim = cosine_similarity(vec.reshape(1,-1), ov.reshape(1,-1))[0][0]\n",
    "        if sim >= threshold:\n",
    "            return orig\n",
    "    return None\n",
    "\n",
    "\n",
    "TOOL_KEYWORDS = [\"calculate\",\"sentiment\",\"pandas\",\"data\"]\n",
    "def is_tool_query(txt: str) -> bool:\n",
    "    low = txt.lower().strip()\n",
    "    return any(low.startswith(kw) for kw in TOOL_KEYWORDS)\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 4) LangGraph router for those 3 tools\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "class RouterState(TypedDict):\n",
    "    input: str\n",
    "    decision: str\n",
    "    output: Any\n",
    "\n",
    "def llm_call_router(state: RouterState) -> RouterState:\n",
    "    t = state[\"input\"].strip().lower()\n",
    "    if t.startswith(\"calculate\"):\n",
    "        state[\"decision\"] = \"Calculator\"\n",
    "    elif any(w in t for w in (\"happy\",\"sad\",\"sentiment\")):\n",
    "        state[\"decision\"] = \"SentimentAnalysis\"\n",
    "    else:\n",
    "        state[\"decision\"] = \"PandasDataAnalysis\"\n",
    "    return state\n",
    "\n",
    "def run_Calculator(s):    s[\"output\"]=calculator_tool(s[\"input\"]); return s\n",
    "def run_Sentiment(s):     s[\"output\"]=sentiment_analysis_tool(s[\"input\"]); return s\n",
    "def run_Pandas(s):        s[\"output\"]=run_pandas(s[\"input\"]);            return s\n",
    "\n",
    "router = StateGraph(state_schema=RouterState)\n",
    "router.add_node(\"router\", llm_call_router)\n",
    "router.add_node(\"Calculator\", run_Calculator)\n",
    "router.add_node(\"SentimentAnalysis\", run_Sentiment)\n",
    "router.add_node(\"PandasDataAnalysis\", run_Pandas)\n",
    "\n",
    "router.add_edge(START, \"router\")\n",
    "router.add_conditional_edges(\"router\", lambda s: s[\"decision\"], {\n",
    "    \"Calculator\":\"Calculator\",\n",
    "    \"SentimentAnalysis\":\"SentimentAnalysis\",\n",
    "    \"PandasDataAnalysis\":\"PandasDataAnalysis\"\n",
    "})\n",
    "for node in (\"Calculator\",\"SentimentAnalysis\",\"PandasDataAnalysis\"):\n",
    "    router.add_edge(node, END)\n",
    "\n",
    "router.set_entry_point(\"router\")\n",
    "router_workflow = router.compile()\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 5) Pure‐chat Llama chain + memory\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful AI assistant.\"),\n",
    "    (\"system\",\"Chat history:\\n{chat_history}\"),\n",
    "    (\"system\",\"Long-term memory:\\n{long_term_memory}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\",\"{input}\"),\n",
    "])\n",
    "\n",
    "short_term = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"input\",\n",
    "    return_messages=True\n",
    ")\n",
    "long_term = CombinedMemory([short_term, LongTermChatMemory(session_id=SESSION_ID)])\n",
    "\n",
    "chat_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hf_llm,\n",
    "    memory=long_term\n",
    ")\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 6) Unified converse()\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "def converse(user_input: str) -> str:\n",
    "    low = user_input.lower().strip()\n",
    "    if \"remember\" in low:\n",
    "        return \"Memory:\\n\" + get_long_term_memory(SESSION_ID)\n",
    "\n",
    "    if is_tool_query(user_input):\n",
    "        # semantic cache\n",
    "        sim = find_similar_cached(user_input)\n",
    "        if sim:\n",
    "            return _qa_cache[sim]\n",
    "        st  = router_workflow.invoke({\"input\": user_input})\n",
    "        out = st[\"output\"]\n",
    "        _qa_cache[user_input]    = out\n",
    "        _qa_embeds[user_input]   = EMB_MODEL.encode(user_input, convert_to_numpy=True)\n",
    "        update_long_term_memory(SESSION_ID, user_input, out)\n",
    "        return out\n",
    "\n",
    "    # fallback to pure chat\n",
    "    out = chat_chain.run(user_input)\n",
    "    update_long_term_memory(SESSION_ID, user_input, out)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 7) Smoke‐test\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for q in [\n",
    "        \"Calculate 3 * 8\",\n",
    "        \"How am I feeling? Sentiment 'I am sad'\",\n",
    "        \"Show me summary stats on the DataFrame\",\n",
    "        \"Do you remember our previous requests?\"\n",
    "    ]:\n",
    "        print(\"→\", converse(q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99718ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, warnings\n",
    "from typing import TypedDict, Any\n",
    "from transformers import pipeline, logging as tf_logging\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# — Silence warnings —\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf_logging.set_verbosity_error()\n",
    "\n",
    "# ——— State schema ———\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    decision: str\n",
    "    output: Any\n",
    "\n",
    "# ——— In‑memory stores ———\n",
    "short_term_memory: list[str] = []\n",
    "long_term_memory:  list[str] = []\n",
    "\n",
    "# ——— LLM wrapper ———\n",
    "def generate_chat_template(model, tokenizer, sys_pmt: str, prompt: str,\n",
    "                           temperature=0.0, top_p=0.9,\n",
    "                           max_new_tokens=100, do_sample=False) -> str:\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": sys_pmt},\n",
    "        {\"role\": \"user\",   \"content\": prompt},\n",
    "    ]\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "    formatted = pipe.tokenizer.apply_chat_template(msgs,\n",
    "                                                   tokenize=False,\n",
    "                                                   add_generation_prompt=True)\n",
    "    out = pipe(formatted,\n",
    "               max_new_tokens=max_new_tokens,\n",
    "               do_sample=do_sample,\n",
    "               temperature=temperature,\n",
    "               top_p=top_p)\n",
    "    return out[0][\"generated_text\"].split(formatted, 1)[1].strip()\n",
    "\n",
    "# ——— Tools ———\n",
    "def calculator(state: State) -> State:\n",
    "    expr = re.sub(r'(?i)^calculate\\s*', '', state[\"input\"]).strip()\n",
    "    try:\n",
    "        result = eval(expr, {\"__builtins__\":None}, {})\n",
    "    except Exception:\n",
    "        result = \"Error\"\n",
    "    state[\"output\"] = result\n",
    "    short_term_memory.append(str(result))\n",
    "    return state\n",
    "\n",
    "def chat(state: State) -> State:\n",
    "    reply = generate_chat_template(\n",
    "        model, tokenizer,\n",
    "        sys_pmt=\"You are a concise assistant.\",\n",
    "        prompt=state[\"input\"],\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    state[\"output\"] = reply\n",
    "    short_term_memory.append(reply)\n",
    "    return state\n",
    "\n",
    "def short_term(state: State) -> State:\n",
    "    # last 5 entries\n",
    "    history = short_term_memory[-5:]\n",
    "    state[\"output\"] = \"\\n\".join(history)\n",
    "    return state\n",
    "\n",
    "def long_term(state: State) -> State:\n",
    "    txt = state[\"input\"].strip()\n",
    "    # match both “Remember X” and “Remember the following: X”\n",
    "    m = re.match(r'(?i)^remember(?: the following)?:\\s*(.+)', txt)\n",
    "    if m:\n",
    "        fact = m.group(1).strip()\n",
    "        long_term_memory.append(fact)\n",
    "        state[\"output\"] = \"\"        # silent ack\n",
    "    else:\n",
    "        state[\"output\"] = \"\\n\".join(long_term_memory)\n",
    "    return state\n",
    "\n",
    "# ——— Router ———\n",
    "def llm_call_router(state: State) -> State:\n",
    "    txt = state[\"input\"].strip()\n",
    "    low = txt.lower()\n",
    "\n",
    "    # explicit memory commands\n",
    "    if low.startswith(\"remember\"):\n",
    "        state[\"decision\"] = \"long_term\"\n",
    "        return state\n",
    "    if re.match(r'(?i)^what is my name\\??', txt):\n",
    "        state[\"decision\"] = \"long_term\"\n",
    "        return state\n",
    "    if low.startswith(\"show recent\") or low.startswith(\"what did we do recently\"):\n",
    "        state[\"decision\"] = \"short_term\"\n",
    "        return state\n",
    "\n",
    "    # explicit calculator\n",
    "    if low.startswith(\"calculate\") or re.fullmatch(r\"[\\d\\.\\s\\+\\-\\*\\/\\^\\(\\)]+\", txt):\n",
    "        state[\"decision\"] = \"calculator\"\n",
    "        return state\n",
    "\n",
    "    # fallback: ask llama\n",
    "    choice = generate_chat_template(\n",
    "        model, tokenizer,\n",
    "        sys_pmt=\"Classify this as exactly 'calculator' or 'chat'.\",\n",
    "        prompt=txt,\n",
    "        temperature=0.0,\n",
    "        top_p=0.9,\n",
    "        do_sample=False\n",
    "    ).lower()\n",
    "    state[\"decision\"] = \"calculator\" if \"calculator\" in choice else \"chat\"\n",
    "    return state\n",
    "\n",
    "def route_decision(state: State) -> str:\n",
    "    return {\n",
    "        \"calculator\": \"CALC\",\n",
    "        \"chat\":       \"CHAT\",\n",
    "        \"short_term\":\"SHORT\",\n",
    "        \"long_term\": \"LONG\",\n",
    "    }[state[\"decision\"]]\n",
    "\n",
    "# ——— Build & compile ———\n",
    "router = StateGraph(State)\n",
    "router.add_node(\"CALC\",    calculator)\n",
    "router.add_node(\"CHAT\",    chat)\n",
    "router.add_node(\"SHORT\",   short_term)\n",
    "router.add_node(\"LONG\",    long_term)\n",
    "router.add_node(\"ROUTER\",  llm_call_router)\n",
    "\n",
    "router.add_edge(START, \"ROUTER\")\n",
    "router.add_conditional_edges(\n",
    "    \"ROUTER\",\n",
    "    route_decision,\n",
    "    {\"CALC\":\"CALC\",\"CHAT\":\"CHAT\",\"SHORT\":\"SHORT\",\"LONG\":\"LONG\"},\n",
    ")\n",
    "router.add_edge(\"CALC\",  END)\n",
    "router.add_edge(\"CHAT\",  END)\n",
    "router.add_edge(\"SHORT\", END)\n",
    "router.add_edge(\"LONG\",  END)\n",
    "\n",
    "router.set_entry_point(\"ROUTER\")\n",
    "router_workflow = router.compile()\n",
    "\n",
    "# ——— Test harness ———\n",
    "tests = [\n",
    "    \"Calculate 3 * 8\",\n",
    "    \"Remember the following: my name is Lucius Vila Volum\",\n",
    "    \"What is my name?\",\n",
    "    \"Show recent\",\n",
    "    \"Tell me a joke\"\n",
    "]\n",
    "\n",
    "for q in tests:\n",
    "    state = router_workflow.invoke({\"input\": q})\n",
    "    out = state[\"output\"] or \"(blank)\"\n",
    "    print(f\">>> {q!r}\")\n",
    "    print(\"-> Routed to:\", state[\"decision\"])\n",
    "    print(\"-> Output:\\n\", out, \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
